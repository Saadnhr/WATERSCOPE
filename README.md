# ğŸŒŠ WaterScope - Inland Water Body Dynamics Monitor

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![Elasticsearch 8.11](https://img.shields.io/badge/elasticsearch-8.11-yellow.svg)](https://www.elastic.co/)
[![Tests](https://img.shields.io/badge/tests-24%20passed-green.svg)](tests/)
[![Coverage](https://img.shields.io/badge/coverage-77%25-brightgreen.svg)](htmlcov/)

**Projet B3 - IPSA Paris**  
*Surveillance automatisÃ©e des masses d'eau par donnÃ©es satellite Sentinel-2*

Auteur : Saad Nhari | Janvier 2026

---

## ğŸ“– Description

WaterScope surveille automatiquement l'Ã©volution des lacs et rÃ©servoirs en utilisant des images satellite Sentinel-2 gratuites. Le systÃ¨me calcule la surface en eau via l'indice NDWI (Normalized Difference Water Index) et stocke les rÃ©sultats dans Elasticsearch pour analyse dans Kibana.

**ğŸ¯ Objectif :** DÃ©tecter les variations de surface des masses d'eau pour identifier les sÃ©cheresses et analyser les tendances climatiques.

---

## ğŸš€ Quick Start (5 minutes)

```powershell
# 1. Activer l'environnement virtuel
.\venv\Scripts\Activate.ps1

# 2. DÃ©marrer Docker
docker-compose up -d

# 3. Attendre 30 secondes, puis vÃ©rifier
curl http://localhost:9200

# 4. Lancer l'ingestion
python scripts\test_full_pipeline.py

# 5. Ouvrir Kibana
start http://localhost:5601
```

---

## ğŸ› ï¸ Installation ComplÃ¨te

### PrÃ©requis
- âœ… Docker Desktop installÃ© et dÃ©marrÃ©
- âœ… Python 3.11 ou supÃ©rieur
- âœ… Compte Copernicus Data Space (gratuit : https://dataspace.copernicus.eu/)

### Ã‰tape 1 : Environnement Python

```powershell
# Naviguer vers le dossier du projet
cd "C:\Users\...\WATERSCOPE"

# CrÃ©er l'environnement virtuel
python -m venv venv

# Activer l'environnement (Windows PowerShell)
.\venv\Scripts\Activate.ps1

# Activer l'environnement (Windows CMD)
venv\Scripts\activate.bat

# VÃ©rifier l'activation (doit afficher le chemin de venv)
where python
```

### Ã‰tape 2 : Installer les DÃ©pendances

```powershell
# Installer toutes les dÃ©pendances
pip install -r requirements.txt

# VÃ©rifier que tout est bien installÃ©
pip list

# VÃ©rifier version Elasticsearch (DOIT Ãªtre 8.11.1)
pip show elasticsearch
```

### Ã‰tape 3 : Configuration

```powershell
# CrÃ©er le fichier .env Ã  la racine
New-Item -ItemType File -Path ".env"

# Ouvrir .env avec un Ã©diteur
notepad .env
```

**Contenu du .env :**
```env
COPERNICUS_USERNAME=ton_email@ipsa.fr
COPERNICUS_PASSWORD=ton_mot_de_passe
ELASTICSEARCH_HOST=http://localhost:9200
KIBANA_HOST=http://localhost:5601
PYTHONPATH=./src
```

### Ã‰tape 4 : DÃ©marrer Docker

```powershell
# DÃ©marrer Elasticsearch + Kibana
docker-compose up -d

# VÃ©rifier que les conteneurs tournent
docker ps

# Devrait afficher :
# waterscope_elasticsearch (port 9200)
# waterscope_kibana (port 5601)

# Attendre 30-60 secondes que tout dÃ©marre
Start-Sleep -Seconds 30

# VÃ©rifier Elasticsearch
curl http://localhost:9200

# Devrait retourner du JSON avec "You Know, for Search"
```

### Ã‰tape 5 : Test Initial

```powershell
# Tester l'authentification
python src\sentinel_hub\auth.py

# Devrait afficher : "Token obtained successfully!"

# Tester Elasticsearch
python src\elasticsearch_client\client.py

# Devrait afficher : "Connected to Elasticsearch"
```

### Ã‰tape 6 : PremiÃ¨re Ingestion

```powershell
# Lancer le pipeline complet
python scripts\test_full_pipeline.py

# Devrait afficher :
# Processing: Lake Aral (lake_aral_001) âœ…
# Processing: Lake Chad (lake_chad_001) âœ…
# Processing: Lake Mead (lake_mead_001) âœ…
# Total: 3 | Successful: 3 | Failed: 0
```

### Ã‰tape 7 : VÃ©rification dans Kibana

```powershell
# Ouvrir Kibana dans le navigateur
start http://localhost:5601

# VÃ©rifier le nombre de documents
curl http://localhost:9200/waterbody_stats/_count

# Devrait retourner : {"count": 3, ...}
```

**Dans Kibana :**
1. Menu â˜° â†’ Management â†’ Stack Management
2. Data â†’ Index Patterns â†’ Create index pattern
3. Name: `waterbody_stats*`
4. Time field: `timestamp`
5. Create
6. Menu â˜° â†’ Analytics â†’ Discover
7. Changer time range en haut Ã  droite : "Last 1 year"
8. Voir les 3 documents !

---

## ğŸ® Commandes Essentielles

### ğŸ”§ Gestion de l'Environnement

```powershell
# Activer l'environnement virtuel
.\venv\Scripts\Activate.ps1

# DÃ©sactiver l'environnement
deactivate

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt

# Mettre Ã  jour une dÃ©pendance spÃ©cifique
pip install --upgrade requests

# Lister les packages installÃ©s
pip list

# Voir les packages obsolÃ¨tes
pip list --outdated
```

### ğŸ³ Gestion Docker

```powershell
# DÃ©marrer les services
docker-compose up -d

# ArrÃªter les services
docker-compose down

# RedÃ©marrer les services
docker-compose restart

# Voir les conteneurs en cours
docker ps

# Voir tous les conteneurs (y compris arrÃªtÃ©s)
docker ps -a

# Voir les logs d'Elasticsearch
docker logs waterscope_elasticsearch

# Voir les logs de Kibana
docker logs waterscope_kibana

# Suivre les logs en temps rÃ©el
docker-compose logs -f

# Supprimer tout (y compris donnÃ©es!)
docker-compose down -v
```

### ğŸ§ª Tests

```powershell
# Lancer TOUS les tests
pytest tests/ -v

# Tests avec coverage
pytest tests/ -v --cov=src

# Coverage avec rapport dÃ©taillÃ©
pytest tests/ -v --cov=src --cov-report=term-missing

# GÃ©nÃ©rer rapport HTML
pytest tests/ -v --cov=src --cov-report=html

# Ouvrir le rapport dans le navigateur
start htmlcov/index.html

# Tests d'un module spÃ©cifique
pytest tests/test_sentinel_hub/test_auth.py -v
pytest tests/test_sentinel_hub/test_process_api.py -v
pytest tests/test_elasticsearch/test_client.py -v

# Tests sans intÃ©gration
pytest tests/ -v -m "not integration"

# Test d'une fonction spÃ©cifique
pytest tests/test_auth.py::TestSentinelHubAuth::test_init_avec_credentials -v
```

### ğŸš€ ExÃ©cution du Pipeline

```powershell
# Pipeline complet (tous les lacs)
python scripts\test_full_pipeline.py

# Tester uniquement l'authentification
python src\sentinel_hub\auth.py

# Tester uniquement le Process API
python src\sentinel_hub\process_api.py

# Tester uniquement Elasticsearch
python src\elasticsearch_client\client.py

# Tester le service d'ingestion
python src\ingestion\ingestion_service.py
```

### â° Scheduler Automatique

```powershell
# ExÃ©cution immÃ©diate (pour tester)
python src\ingestion\scheduler.py --mode now

# ExÃ©cution quotidienne Ã  14:00 (dÃ©veloppement)
python src\ingestion\scheduler.py --mode daily

# ExÃ©cution mensuelle le 1er Ã  02:00 (production)
python src\ingestion\scheduler.py --mode monthly

# Avec fichier de config personnalisÃ©
python src\ingestion\scheduler.py --config config/custom_lakes.json --mode now
```

### ğŸ” VÃ©rifications Elasticsearch

```powershell
# SantÃ© du cluster
curl http://localhost:9200/_cluster/health

# Compter les documents
curl http://localhost:9200/waterbody_stats/_count

# Voir tous les documents (10 premiers)
curl http://localhost:9200/waterbody_stats/_search

# Voir le mapping de l'index
curl http://localhost:9200/waterbody_stats/_mapping

# Liste de tous les indices
curl http://localhost:9200/_cat/indices?v

# Statistiques de l'index
curl http://localhost:9200/waterbody_stats/_stats

# Supprimer l'index (ATTENTION: perte de donnÃ©es!)
curl -X DELETE http://localhost:9200/waterbody_stats
```

### ğŸ“Š Kibana

```powershell
# Ouvrir Kibana
start http://localhost:5601

# Ouvrir directement Discover
start http://localhost:5601/app/discover

# Ouvrir Stack Management
start http://localhost:5601/app/management
```

### ğŸ› Debugging

```powershell
# Voir les logs du scheduler (si exÃ©cutÃ©)
type data\logs\scheduler.log

# Nettoyer les fichiers cache Python
Get-ChildItem -Path . -Recurse -Filter "__pycache__" | Remove-Item -Recurse -Force
Get-ChildItem -Path . -Recurse -Filter "*.pyc" | Remove-Item -Force

# VÃ©rifier les processus Python en cours
Get-Process python

# Tuer un processus Python
Stop-Process -Name python

# Entrer dans le conteneur Elasticsearch
docker exec -it waterscope_elasticsearch bash

# Voir l'utilisation des ressources Docker
docker stats
```

### ğŸ“ Gestion de Fichiers

```powershell
# Voir l'arborescence du projet
tree /F

# CrÃ©er les dossiers manquants
mkdir docs\screenshots
mkdir data\logs

# Compter les lignes de code
Get-ChildItem -Path src -Recurse -Filter *.py | Get-Content | Measure-Object -Line

# Compter les fichiers Python
Get-ChildItem -Path src -Recurse -Filter *.py | Measure-Object
```

---

## ğŸ¯ ScÃ©narios d'Usage

### ScÃ©nario 1 : Premier DÃ©marrage du Jour

```powershell
# 1. Activer l'environnement
.\venv\Scripts\Activate.ps1

# 2. DÃ©marrer Docker (si pas dÃ©jÃ  fait)
docker-compose up -d

# 3. VÃ©rifier que tout fonctionne
curl http://localhost:9200

# 4. Lancer les tests
pytest tests/ -v

# 5. Tu es prÃªt Ã  travailler!
```

### ScÃ©nario 2 : Ajouter un Nouveau Lac

```powershell
# 1. Ã‰diter le fichier de configuration
code config\waterbodies_config.json

# Ajouter :
# {
#   "waterbody_id": "lake_custom_001",
#   "name": "Mon Lac",
#   "geometry": { ... },
#   ...
# }

# 2. Lancer l'ingestion
python scripts\test_full_pipeline.py

# 3. VÃ©rifier dans Kibana
start http://localhost:5601
```

### ScÃ©nario 3 : Tout RÃ©initialiser

```powershell
# 1. ArrÃªter Docker
docker-compose down -v

# 2. Supprimer l'environnement virtuel
Remove-Item -Recurse -Force venv

# 3. RecrÃ©er tout
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
docker-compose up -d
Start-Sleep -Seconds 30
python scripts\test_full_pipeline.py
```

### ScÃ©nario 4 : Avant de Rendre le Projet

```powershell
# 1. Lancer tous les tests
pytest tests/ -v --cov=src --cov-report=html

# 2. VÃ©rifier le coverage (doit Ãªtre >60%)
start htmlcov/index.html

# 3. Lancer le pipeline
python scripts\test_full_pipeline.py

# 4. VÃ©rifier Kibana
start http://localhost:5601

# 5. Prendre les captures d'Ã©cran

# 6. GÃ©nÃ©rer le PDF du rapport
# (Dans VS Code avec extension Markdown PDF)
```

---

## ğŸ§ª Tests

**RÃ©sultats actuels :**
- âœ… 24 tests passÃ©s (8 auth + 10 process_api + 6 elasticsearch)
- âœ… Coverage : 77% (objectif >60%)
- âœ… 0 Ã©checs

```powershell
# Suite de tests complÃ¨te
pytest tests/ -v --cov=src --cov-report=html
```

---

## ğŸ“Š RÃ©sultats

**Pipeline d'ingestion :**
- âœ… 3 lacs configurÃ©s (Lake Aral, Lake Chad, Lake Mead)
- âœ… DonnÃ©es indexÃ©es dans Elasticsearch
- âœ… Visualisations disponibles dans Kibana
- âœ… Temps d'exÃ©cution : ~15 secondes par run

**MÃ©triques de performance :**
- Tests unitaires : 24/24 âœ…
- Coverage : 77%
- Authentification OAuth2 : Fonctionnelle
- Calcul NDWI : ValidÃ© avec seuil 0.2
- Filtrage nuages : SCL 8, 9, 3

---

## ğŸ“ Structure du Projet

```
WATERSCOPE/
â”œâ”€â”€ src/                           # Code source principal
â”‚   â”œâ”€â”€ sentinel_hub/              # Authentification + Process API
â”‚   â”‚   â”œâ”€â”€ auth.py                # OAuth2 token management
â”‚   â”‚   â””â”€â”€ process_api.py         # NDWI calculation client
â”‚   â”œâ”€â”€ elasticsearch_client/      # Client Elasticsearch
â”‚   â”‚   â””â”€â”€ client.py              # CRUD + time-series queries
â”‚   â”œâ”€â”€ ingestion/                 # Service d'ingestion
â”‚   â”‚   â”œâ”€â”€ ingestion_service.py   # Orchestration
â”‚   â”‚   â””â”€â”€ scheduler.py           # Planification automatique
â”‚   â””â”€â”€ utils/                     # Utilitaires
â”‚       â””â”€â”€ logging_config.py      # Configuration logging
â”‚
â”œâ”€â”€ tests/                         # Tests unitaires (Pytest)
â”‚   â”œâ”€â”€ test_sentinel_hub/         # 18 tests
â”‚   â”‚   â”œâ”€â”€ test_auth.py
â”‚   â”‚   â””â”€â”€ test_process_api.py
â”‚   â””â”€â”€ test_elasticsearch/        # 6 tests
â”‚       â””â”€â”€ test_client.py
â”‚
â”œâ”€â”€ config/                        # Configuration
â”‚   â””â”€â”€ waterbodies_config.json    # DÃ©finition des lacs
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ technology_familiarization_report.md  # 40+ pages
â”‚   â””â”€â”€ screenshots/               # Captures Kibana
â”‚
â”œâ”€â”€ scripts/                       # Scripts utilitaires
â”‚   â””â”€â”€ test_full_pipeline.py      # Test intÃ©gration E2E
â”‚
â”œâ”€â”€ docker-compose.yml             # Infrastructure Docker
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â”œâ”€â”€ pytest.ini                     # Configuration tests
â”œâ”€â”€ .env                           # Credentials (non commitÃ©)
â”œâ”€â”€ .gitignore                     # Fichiers Ã  ignorer
â””â”€â”€ README.md                      # Ce fichier
```

---

## ğŸ® Usage AvancÃ©

### Ajouter un Nouveau Lac

Ã‰diter `config/waterbodies_config.json` :

```json
{
  "waterbody_id": "lake_custom_001",
  "name": "Mon Lac PersonnalisÃ©",
  "geometry": {
    "type": "Polygon",
    "coordinates": [[[lon1, lat1], [lon2, lat2], ...]]
  },
  "region": "Europe",
  "country": "France",
  "description": "Description du lac"
}
```

Puis :
```powershell
python scripts\test_full_pipeline.py
```

### RequÃªtes Elasticsearch PersonnalisÃ©es

```powershell
# PowerShell - Chercher par waterbody_id
$body = '{"query":{"term":{"waterbody_id":"lake_aral_001"}}}'
Invoke-WebRequest -Uri "http://localhost:9200/waterbody_stats/_search" -Method POST -ContentType "application/json" -Body $body

# Chercher dans une pÃ©riode
$body = '{"query":{"range":{"timestamp":{"gte":"2024-01-01","lte":"2026-12-31"}}}}'
Invoke-WebRequest -Uri "http://localhost:9200/waterbody_stats/_search" -Method POST -ContentType "application/json" -Body $body
```

---

## ğŸ“ˆ DonnÃ©es Produites

**Index Elasticsearch :** `waterbody_stats`

**Structure des documents :**
```json
{
  "waterbody_id": "lake_aral_001",
  "name": "Lake Aral",
  "timestamp": "2026-01-07T00:00:00Z",
  "surface_area_hectares": 1050.8,
  "data_source": "Sentinel-2",
  "cloud_cover_percentage": 5.2,
  "geo_shape": {
    "type": "Polygon",
    "coordinates": [[[58.5, 45.0], ...]]
  }
}
```

---

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨me : Elasticsearch ne dÃ©marre pas

```powershell
# Voir les logs
docker logs waterscope_elasticsearch

# Solution 1: Augmenter la mÃ©moire Docker
# Docker Desktop â†’ Settings â†’ Resources â†’ Memory â†’ 4GB â†’ Apply

# Solution 2: RedÃ©marrer Docker
docker-compose down
docker-compose up -d
```

### ProblÃ¨me : Erreur 401 Authentification

```powershell
# VÃ©rifier le .env
type .env

# Tester l'authentification
python src\sentinel_hub\auth.py

# Si erreur: vÃ©rifier credentials sur https://dataspace.copernicus.eu/
```

### ProblÃ¨me : Pas de donnÃ©es dans Kibana

```powershell
# 1. VÃ©rifier le nombre de documents
curl http://localhost:9200/waterbody_stats/_count

# 2. Si count = 0, lancer l'ingestion
python scripts\test_full_pipeline.py

# 3. Dans Kibana, changer time range Ã  "Last 1 year"
```

### ProblÃ¨me : Tests Ã©chouent

```powershell
# RÃ©installer elasticsearch Ã  la bonne version
pip uninstall elasticsearch
pip install elasticsearch==8.11.1

# Relancer les tests
pytest tests/ -v
```

### ProblÃ¨me : "Cannot connect to Elasticsearch"

```powershell
# VÃ©rifier qu'Elasticsearch tourne
docker ps

# VÃ©rifier qu'il rÃ©pond
curl http://localhost:9200

# RedÃ©marrer si nÃ©cessaire
docker-compose restart elasticsearch
Start-Sleep -Seconds 30
curl http://localhost:9200
```

---

## ğŸ”§ Configuration

### Variables d'Environnement (.env)

```env
# Copernicus Data Space Credentials
COPERNICUS_USERNAME=your_email@example.com
COPERNICUS_PASSWORD=your_password

# Elasticsearch
ELASTICSEARCH_HOST=http://localhost:9200
KIBANA_HOST=http://localhost:5601

# Python
PYTHONPATH=./src
```

### Docker Compose

Services dÃ©ployÃ©s :
- **Elasticsearch** : Port 9200, 512MB RAM
- **Kibana** : Port 5601

---

## ğŸ“š Documentation

- [ğŸ“„ Rapport Technique Complet](docs/technology_familiarization_report.md) - 40 pages
- [ğŸ“Š Coverage Report](htmlcov/index.html) - GÃ©nÃ©rÃ© par pytest-cov
- [ğŸ§ª Tests](tests/) - 24 tests unitaires
- [ğŸ“¸ Screenshots Kibana](docs/screenshots/) - 5 captures

---

## ğŸ”® Roadmap

### Phase 1 : Infrastructure & Ingestion (Semaines 1-4) âœ…
- [x] Setup Docker (Elasticsearch + Kibana)
- [x] Authentification Sentinel Hub
- [x] Client Process API (NDWI)
- [x] Stockage Elasticsearch
- [x] Tests unitaires (24 tests, 77% coverage)
- [x] Documentation technique

### Phase 2 : API REST (Semaines 5-6) â³
- [ ] API FastAPI
- [ ] Endpoints `/waterbodies/{id}/surface-area`
- [ ] Endpoints `/analytics/drought-risk`
- [ ] Documentation Swagger

### Phase 3 : Dashboard Web (Semaines 7-8) â³
- [ ] Interface web (Streamlit ou React)
- [ ] Graphiques interactifs
- [ ] Alertes automatiques

---

## ğŸ§  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Sentinel-2 Satellite Data           â”‚
â”‚     (Images tous les 5 jours)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Copernicus Data Space / Sentinel Hub  â”‚
â”‚   â€¢ Stockage images satellite           â”‚
â”‚   â€¢ Process API (calculs cloud)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTPS REST API
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python WaterScope Application         â”‚
â”‚   â€¢ auth.py - OAuth2 token              â”‚
â”‚   â€¢ process_api.py - NDWI calculation   â”‚
â”‚   â€¢ ingestion_service.py - Orchestrationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ JSON documents
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Elasticsearch                  â”‚
â”‚   Index: waterbody_stats                â”‚
â”‚   â€¢ Time-series storage                 â”‚
â”‚   â€¢ Queries & Aggregations              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Visualization
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Kibana                      â”‚
â”‚   â€¢ Dashboards                          â”‚
â”‚   â€¢ Time-series charts                  â”‚
â”‚   â€¢ Discover interface                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ MÃ©thodologie NDWI

### Principe Scientifique

Le **NDWI (Normalized Difference Water Index)** exploite les propriÃ©tÃ©s spectrales de l'eau :

**Formule :**
```
NDWI = (B03 - B08) / (B03 + B08)
```

**OÃ¹ :**
- **B03** = Bande verte (560 nm) - RÃ©flÃ©chie par l'eau
- **B08** = Proche infrarouge (842 nm) - AbsorbÃ©e par l'eau

**InterprÃ©tation :**
- `NDWI > 0.2` â†’ Eau
- `NDWI â‰¤ 0.2` â†’ Terre/VÃ©gÃ©tation

### Filtrage des Nuages

Utilisation de **SCL (Scene Classification Layer)** :
- SCL = 3 : Ombre de nuage
- SCL = 8 : Nuage (probabilitÃ© moyenne)
- SCL = 9 : Nuage (probabilitÃ© haute)

---

## âœ… Checklist Quotidienne

```powershell
# Matin
.\venv\Scripts\Activate.ps1
docker-compose up -d
curl http://localhost:9200

# DÃ©veloppement
pytest tests/ -v
python scripts\test_full_pipeline.py

# Fin de journÃ©e
pytest tests/ -v --cov=src --cov-report=term
docker-compose logs > logs_$(Get-Date -Format "yyyyMMdd").txt
```

---

## ğŸ‘¨â€ğŸ’» Auteur

**Saad Nhari**  
B3 - IPSA Paris  
Projet de fin d'Ã©tudes - Janvier 2026

ğŸ“§ saad64547@gmail.com

---

## ğŸ™ Remerciements

- **ESA Copernicus** pour les donnÃ©es Sentinel-2 gratuites
- **Anthropic Claude** pour l'assistance technique
- **IPSA Paris** pour l'encadrement acadÃ©mique
- **Elastic** pour Elasticsearch et Kibana open-source

---

## ğŸ“„ License

Projet acadÃ©mique rÃ©alisÃ© dans le cadre du cursus B3 Ã  IPSA Paris.

Â© 2026 IPSA Paris - Tous droits rÃ©servÃ©s pour usage acadÃ©mique.

---

## ğŸ”— Liens Utiles

- [Documentation Copernicus](https://documentation.dataspace.copernicus.eu/)
- [Sentinel Hub Process API](https://docs.sentinel-hub.com/api/latest/)
- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/8.11/index.html)
- [Kibana Guide](https://www.elastic.co/guide/en/kibana/8.11/index.html)
- [NDWI Scientific Paper](https://doi.org/10.1080/01431169608948714) - McFeeters (1996)

---

**Version :** 1.0.0 (Phase 1 complÃ©tÃ©e)  
**DerniÃ¨re mise Ã  jour :** 8 Janvier 2026  
**Status :** âœ… Production Ready (Phase 1)